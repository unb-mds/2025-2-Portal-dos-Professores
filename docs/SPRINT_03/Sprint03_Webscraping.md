# DecisÃµes TÃ©cnicas Iniciais: Coleta e Processamento de Dados

Este documento resume as decisÃµes tomadas em relaÃ§Ã£o Ã s tecnologias de Web Scraping e SumarizaÃ§Ã£o por InteligÃªncia Artificial para o projeto "Portal de Professores da UnB".

## 1. Contexto e Objetivos

O projeto necessita de um processo automatizado para:
1.  **Coletar** informaÃ§Ãµes pÃºblicas de docentes de fontes como Lattes, Sigaa e Google Scholar.
2.  **Processar** os dados coletados para gerar um resumo qualitativo sobre a atuaÃ§Ã£o de cada professor.
3.  **Operar** de forma eficiente e confiÃ¡vel dentro de uma arquitetura serverless em workflows do GitHub Actions.

Para atender a esses requisitos, realizamos uma anÃ¡lise de ferramentas para cada etapa do processo.

## 2. Escolha da Ferramenta de Web Scraping

Analisamos diversas bibliotecas Python, com foco na capacidade de lidar com sites modernos e na viabilidade de execuÃ§Ã£o no ambiente do GitHub Actions.

### Tabela Comparativa de Ferramentas de Scraping

| Ferramenta | PrÃ³s | Contras | Viabilidade em GitHub Actions |
| :--- | :--- | :--- | :--- |
| **Requests + BS4** | ğŸ”¹ Leve e rÃ¡pido<br>ğŸ”¹ Simples de usar | ğŸ”¸ **NÃ£o executa JavaScript**<br>ğŸ”¸ Ineficaz para sites dinÃ¢micos | **Limitada** |
| **Selenium** | ğŸ”¹ PadrÃ£o de mercado<br>ğŸ”¹ Robusto e confiÃ¡vel | ğŸ”¸ Mais lento que Playwright<br>ğŸ”¸ API mais verbosa | **Excelente** |
| â­ **Playwright** | ğŸ”¹ **RÃ¡pido e moderno**<br>ğŸ”¹ API limpa com suporte `async`<br>ğŸ”¹ **Setup simplificado** | ğŸ”¸ Curva de aprendizado um pouco maior que o bÃ¡sico | **Excelente (Recomendado)** |

### DecisÃ£o: Playwright

A biblioteca **Playwright** foi a escolhida. Sua performance superior, API moderna e, principalmente, sua facilidade de configuraÃ§Ã£o em ambientes de CI/CD como o GitHub Actions a tornam a soluÃ§Ã£o ideal para garantir uma coleta de dados robusta e eficiente.

## 3. Escolha da Ferramenta de SumarizaÃ§Ã£o por IA

A sumarizaÃ§Ã£o dos dados coletados Ã© um requisito chave. Avaliamos duas abordagens principais: rodar um modelo de IA localmente no runner do GitHub Actions ou utilizar uma API externa.

### Tabela Comparativa de Abordagens de SumarizaÃ§Ã£o

| Abordagem | PrÃ³s | Contras | Viabilidade em GitHub Actions |
| :--- | :--- | :--- | :--- |
| **Modelo Local (Hugging Face)** | ğŸ”¹ Sem custo de API<br>ğŸ”¹ Controle total | ğŸ”¸ **Alto consumo de CPU/RAM**<br>ğŸ”¸ Lento, aumenta o tempo do workflow<br>ğŸ”¸ Risco de falhas por limite de recursos | **Baixa** |
| â­ **API Externa (Google Gemini)** | ğŸ”¹ **Leve e rÃ¡pido para o runner**<br>ğŸ”¹ Resumos de altÃ­ssima qualidade<br>ğŸ”¹ **ImplementaÃ§Ã£o simples**<br>ğŸ”¹ Custo zero para a escala do projeto | ğŸ”¸ DependÃªncia externa<br>ğŸ”¸ Requer gerenciamento de chaves de API | **Excelente (Recomendado)** |

### DecisÃ£o: API do Google Gemini

Optamos por utilizar a **API do Google Gemini**. Esta abordagem delega o processamento pesado de IA para um serviÃ§o especializado, mantendo nosso workflow no GitHub Actions extremamente rÃ¡pido, leve e confiÃ¡vel. A qualidade dos resumos Ã© excelente e a cota de uso gratuito Ã© mais do que suficiente para as necessidades do projeto.

## 4. Documento de DecisÃ£o Arquitetural (ADR)

Para formalizar estas escolhas, foi redigido o seguinte Architecture Decision Record (ADR).

---

### ADR-001: Escolha de Ferramentas para Coleta de Dados e SumarizaÃ§Ã£o por IA

* **Status:** `Accepted`

* **Context:** O projeto "Portal de Professores da UnB" necessita de um processo automatizado para coletar dados de fontes web e gerar resumos qualitativos por IA. Este processo deve operar em uma arquitetura serverless (GitHub Actions), sendo robusto para sites dinÃ¢micos e eficiente com os recursos disponÃ­veis.

* **Decision:**
    1.  Para web scraping, utilizaremos a biblioteca **Playwright**.
    2.  Para a geraÃ§Ã£o dos resumos por IA, utilizaremos a **API do Google Gemini**.

* **Consequences:**
    * **Positivas:**
        * Capacidade de extrair dados de sites complexos e dinÃ¢micos.
        * Workflow de dados rÃ¡pido e eficiente no GitHub Actions.
        * Alta qualidade nos resumos gerados com implementaÃ§Ã£o simplificada.
        * ManutenÃ§Ã£o do "backend" focada em scripts Python, sem a complexidade de gerenciar modelos de IA.
    * **Negativas:**
        * Introduz uma dependÃªncia externa (API do Google Gemini).
        * Exige o gerenciamento seguro de chaves de API atravÃ©s do GitHub Secrets.
        * Potencial de custo futuro em caso de expansÃ£o massiva do projeto.

---

## 5. POC mÃ­nima

### A ideia da POC Ã© mostrar um exemplo mÃ­nimo de como rodaria o cÃ³digo em conjunto da biblioteca **Playwright** para a raspagem dos dados e integrando com os **Agentes de IA** do **Gemini**.

### Link do script que roda a POC mÃ­nima: [POC (script)](https://github.com/unb-mds/2025-2-Portal-dos-Professores/blob/main/docs/SPRINT_03/Sprint03_POC_minima.py)
### Guia RÃ¡pido: Rodar a POC Localmente

Guia para configurar e rodar o script da Prova de Conceito (POC) sem Docker.

### PrÃ©-requisitos
- Python 3.9+ e Git instalados.

---

### 1. Preparar o Ambiente

No seu terminal, clone o repositÃ³rio, entre na pasta, crie e ative um ambiente virtual.

```bash
# Clone o projeto e entre na pasta
git clone <URL_DO_REPOSITORIO>
cd <NOME_DO_PROJETO>

# Crie e ative o ambiente virtual
python -m venv .venv

# No Windows (PowerShell):
.venv\Scripts\Activate.ps1

# No Linux/macOS:
source .venv/bin/activate
```

### 2. Instalar DependÃªncias

Com o ambiente (.venv) ativado, instale os pacotes necessÃ¡rios.

```bash
# Instalar bibliotecas Python
pip install -r requirements.txt

# Instalar navegadores do Playwright
playwright install
```

### 3. Configurar a Chave de API

VocÃª precisa de uma chave secreta do Google para rodar o script.

Copie o arquivo de exemplo para criar seu arquivo de segredos local.

- No Windows: copy .env.example .env
- No Linux/macOS: cp .env.example .env

Gere sua chave no [Google AI Studio](https://aistudio.google.com/).

Cole a chave dentro do arquivo .env que vocÃª acabou de criar.

### 4. Executar o Script
Execute a POC com o seguinte comando:

```bash
python poc.py
```

VocÃª verÃ¡ o texto original da Wikipedia e o resumo gerado pela IA no terminal.

---